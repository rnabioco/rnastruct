shell.executable("/bin/bash")
shell.prefix("source ~/.bash_profile; ")
import glob
import os

''' rules to normalize enzymatic probing data ''' 

configfile: "config.yaml"

RAW_DATA=config["RAW_DATA"]  
DATA = config["DATA"]
dbases = "../../dbases/"
SAMPLES = config["SAMPLES"]
CHROMSIZES = config["CHROMSIZES"]
CHROMS = config["CHROMS"]
CONTAMINATION_BED = config["CONTAMINATION_BED"]
SAMPLE_MAP = config["SAMPLE_MAP"]
LIB_MAP = config["LIB_MAP"]

ASSAYS = ["total_nascent_RNA",
          "ssRNA",
          "dsRNA"]
NORM_METHODS = [
  "norm_coverage",
  "norm_nascent",
  "norm_counts"
]

rule all:
  input:
    expand("{data}/split_bams/{sample}/{assay}.{strand}.bam",
      data = DATA, sample = SAMPLES, strand = ["fwd", "rev"], assay = ASSAYS),
    expand("{data}/norm_factors/{sample}/{assay}.txt",
      data = DATA, sample = SAMPLES, assay = ASSAYS), 
    #expand("{data}/norm_bw/{samples}/{samples}_{rna_type}_{chrom}_{strand}.bw",
    #  data = DATA, samples = SAMPLES, strand = ["fwd", "rev"], chrom =
    #  CHROMS, rna_type = ["dsRNA", "ssRNA"]),


    expand("{data}/struct_score/{method}/{sample}/{sample}_{chrom}_{strand}.tsv.gz",
      data = DATA, sample = SAMPLES, strand = ["fwd", "rev"], chrom =
      CHROMS, method = NORM_METHODS),
    #
    expand("{data}/norm/{sample}/{sample}_{chrom}_{strand}.npz",
      data = DATA, sample = SAMPLES, strand = ["fwd", "rev"], chrom = CHROMS),
    expand("{data}/norm/{sample}/{sample}_{chrom}_{strand}.tsv.gz",
      data = DATA, sample = SAMPLES, strand = ["fwd", "rev"], chrom = CHROMS),

    expand("{data}/struct_score/{method}/{sample}/single_files/{sample}.tsv.gz",
      data = DATA, sample = SAMPLES,
      method = NORM_METHODS),

#    expand("{data}/struct_score/merged_strand/{sample}/{sample}_{chrom}.tsv.gz",
#      data = DATA, sample = SAMPLES, chrom = CHROMS),

rule deeptools_norm_factors:
    """
    generate normalization factors for each bam 
    """
    input:
      lambda wildcards: os.path.join(RAW_DATA, wildcards.sample + "_" + SAMPLE_MAP[wildcards.assay] + ".bam")

    output:
      bt = "{data}/norm_factors/{sample}/{assay}.txt",
      cov = "{data}/norm_factors/{sample}/{assay}_coverage.txt",
    params:
      job_name = "deeptools_norm.{sample}",
      memory = "select[mem>75] rusage[mem=75] span[hosts=1]",
      chroms = dbases + "/chrom_bed.bed",
      prefix =  "{data}/norm_factors/{sample}/{assay}"
    log:
      "log/norm_factors/{sample}.txt"
    threads: 12 
    resources: all_threads=12
    shell:
      """
      estimateReadFiltering \
        -b {input} \
        -p {threads} \
        -bl {CONTAMINATION_BED} \
        > {output.bt}
      
      mosdepth \
        -n \
        -T 1 \
        -x \
        -b {params.chroms} \
        -t 4 \
        {params.prefix} \
        {input}

      gunzip -c {params.prefix}.thresholds.bed.gz \
        | awk '{{x+=$5}} END {{print x}}' \
        > {output.cov}

      """
def _get_all_chroms(wildcards):
    out = expand("{data}/struct_score/{method}/merged_strand/{sample}/{sample}_{chrom}.tsv.gz",
                   chrom = CHROMS, 
                   data = wildcards.data, 
                   sample = wildcards.sample,
                   method = wildcards.method)
                   
    return out

rule combine_all:
    input:
      _get_all_chroms
    output:
      "{data}/struct_score/{method}/{sample}/single_files/{sample}.tsv.gz",
    params:
      job_name = "combine.{sample}",
      memory = "select[mem>20] rusage[mem=20] span[hosts=1]",
    log:
      "log/combine/{method}/{sample}.txt"
    threads: 10
    resources: all_threads=10
    shell:
      """
      cat {input} \
        | gunzip -c \
        | sort --parallel=8 -S 1G -k1,1 -k2,2n -k3,3n \
        | bgzip > {output} 
      
      tabix -0 -s 1 -b 2 -e 3 {output}

      """

rule combine_scores:
    input:
      fwd = "{data}/struct_score/{method}/{sample}/{sample}_{chrom}_fwd.tsv.gz",
      rev = "{data}/struct_score/{method}/{sample}/{sample}_{chrom}_rev.tsv.gz",
    output:
      temp("{data}/struct_score/{method}/merged_strand/{sample}/{sample}_{chrom}.tsv.gz"),
    params:
      job_name = "combine.{sample}",
      memory = "select[mem>25] rusage[mem=25] span[hosts=1]",
    log:
      "log/combine/{method}/{sample}_{chrom}.txt"
    threads: 3
    resources: all_threads=3
    shell:
      """
      cat \
        <(gunzip -c {input.fwd} \
          | awk '{{OFS=FS="\t"}}{{print $0,"+"}}') \
        <(gunzip -c {input.rev} \
          | awk '{{OFS=FS="\t"}}{{print $0,"-"}}') \
        | bedtools sort -i - \
        | gzip > {output}
      """

rule structure_score_nascent:
    """
    calculate structure scores using nascent RNA as normalizer
    """
    input:
      total = "{data}/norm_factors/{sample}/" + ASSAYS[0] + ".txt",
      ssrna =
      "{data}/norm_factors/{sample}/" + ASSAYS[1] + ".txt",
      dsrna = "{data}/norm_factors/{sample}/" + ASSAYS[2] + ".txt",
      bed = "{data}/norm/{sample}/{sample}_{chrom}_{strand}.tsv.gz",
    output:
      bed = "{data}/struct_score/norm_nascent/{sample}/{sample}_{chrom}_{strand}.tsv.gz",
    params:
      job_name = "structure_score.{sample}",
      memory = "select[mem>5] rusage[mem=5] span[hosts=1]",
    log:
      "log/structure_summary/{sample}_{strand}_{chrom}.txt"
    threads: 2
    resources: all_threads=2
    run:
      norm_files = [input.total, 
                    input.ssrna,
                    input.dsrna]
      norm_vals = []
      for sample in norm_files:
        with open(sample) as f:
          hdr = f.readline()
          vals = f.readline().split()
          reads_mapped = int(vals[2])
          reads_to_ignore = int(vals[3])
          read_count = reads_mapped - reads_to_ignore
          norm_vals.append(read_count)

      norm_vals = [str(x) for x in norm_vals]
      norm_vals = ",".join(norm_vals)
      print(norm_vals, file = sys.stderr)

      cmd = "python3 structure_score.py -c {input.bed} -r "
      cmd += norm_vals
      cmd += " -m nascent "
      cmd += " | gzip > {output}"
      shell(cmd)

rule structure_score_counts:
    """
    calculate structure scores using read counts as normalizer 
    """
    input:
      total = "{data}/norm_factors/{sample}/" + ASSAYS[0] + ".txt",
      ssrna =
      "{data}/norm_factors/{sample}/" + ASSAYS[1] + ".txt",
      dsrna = "{data}/norm_factors/{sample}/" + ASSAYS[2] + ".txt",
      bed = "{data}/norm/{sample}/{sample}_{chrom}_{strand}.tsv.gz",
    output:
      bed = "{data}/struct_score/norm_counts/{sample}/{sample}_{chrom}_{strand}.tsv.gz",
    params:
      job_name = "structure_score.{sample}",
      memory = "select[mem>5] rusage[mem=5] span[hosts=1]",
    log:
      "log/structure_summary/{sample}_{strand}_{chrom}.txt"
    threads: 2
    resources: all_threads=2
    run:
      norm_files = [input.total, 
                    input.ssrna,
                    input.dsrna]
      norm_vals = []
      for sample in norm_files:
        with open(sample) as f:
          hdr = f.readline()
          vals = f.readline().split()
          reads_mapped = int(vals[2])
          reads_to_ignore = int(vals[3])
          read_count = reads_mapped - reads_to_ignore
          norm_vals.append(read_count)

      norm_vals = [str(x) for x in norm_vals]
      norm_vals = ",".join(norm_vals)
      print(norm_vals, file = sys.stderr)

      cmd = "python3 structure_score.py -c {input.bed} -r "
      cmd += norm_vals
      cmd += " -m counts "
      cmd += " | gzip > {output}"
      shell(cmd)



rule structure_score_coverage:
    """
    calculate structure scores using read coverage as normalizer 
    """
    input:
      total = "{data}/norm_factors/{sample}/" + ASSAYS[0] + "_coverage.txt",
      ssrna =
      "{data}/norm_factors/{sample}/" + ASSAYS[1] + "_coverage.txt",
      dsrna = "{data}/norm_factors/{sample}/" + ASSAYS[2] + "_coverage.txt",
      bed = "{data}/norm/{sample}/{sample}_{chrom}_{strand}.tsv.gz",
    output:
      bed = "{data}/struct_score/norm_coverage/{sample}/{sample}_{chrom}_{strand}.tsv.gz",
    params:
      job_name = "structure_score.{sample}",
      memory = "select[mem>5] rusage[mem=5] span[hosts=1]",
    log:
      "log/structure_summary/{sample}_{strand}_{chrom}.txt"
    threads: 2
    resources: all_threads=2
    run:
      norm_files = [input.total, 
                    input.ssrna,
                    input.dsrna]
      norm_vals = []
      for sample in norm_files:
        with open(sample) as f:
          read_coverage = int(f.readline().strip())
          norm_vals.append(read_coverage)

      norm_vals = [str(x) for x in norm_vals]
      norm_vals = ",".join(norm_vals)
      print(norm_vals, file = sys.stderr)

      cmd = "python3 structure_score.py -c {input.bed} -r "
      cmd += norm_vals
      cmd += " -m coverage "
      cmd += " | gzip > {output}"
      shell(cmd)

rule deeptools_bamsummary:
    """
    generate per nucleotide summaries of coverage for multiple bams
    """
    input:
      total =
      "{data}/split_bams/{sample}/" + ASSAYS[0] + ".{strand}.bam",
      ssrna =
      "{data}/split_bams/{sample}/" + ASSAYS[1] + ".{strand}.bam",
      dsrna =
      "{data}/split_bams/{sample}/" + ASSAYS[2] + ".{strand}.bam",
    output:
      npz = "{data}/norm/{sample}/{sample}_{chrom}_{strand}.npz",
      raw = "{data}/norm/{sample}/{sample}_{chrom}_{strand}.tsv.gz",
    params:
      raw = "{data}/norm/{sample}/{sample}_{chrom}_{strand}.tsv",
      job_name = "deeptools_norm.{sample}",
      memory = "select[mem>75] rusage[mem=75] span[hosts=1]",
    log:
      "log/norm_summary/{sample}_{strand}_{chrom}.txt"
    threads:12
    resources: all_threads=12
    shell:
      """
      multiBamSummary \
        bins \
        -b {input.total} {input.ssrna} {input.dsrna} \
        -bs 1 \
        --region {wildcards.chrom} \
        -p {threads} \
        -v \
        -out {output.npz} \
        --outRawCounts {params.raw} 

      awk 'NR > 1 && $4 + $5 + $6 != 0' {params.raw} \
        | sort --parallel={threads} -S 1G -k1,1 -k2,2n -k3,3n - \
        | gzip > {output.raw}
      
      rm {params.raw}
      """

def _pick_split_script(wildcards):
    
    if LIB_MAP[wildcards.assay] == "fr-firststrand":
      split_script = "split_bams_frfirst.sh"
    elif LIB_MAP[wildcards.assay] == "fr-secondstrand":
      split_script = "split_bams_frsecond.sh"
    else:
      sys.exit("unknown librartype specific in config")

    return split_script
rule split_bams:
    """
    split bams into alignments from forward or reverse strand
    """
    input:
      lambda wildcards: os.path.join(RAW_DATA, wildcards.sample + "_" + SAMPLE_MAP[wildcards.assay] + ".bam")
    output:
      fwd = "{data}/split_bams/{sample}/{assay}.fwd.bam",
      rev = "{data}/split_bams/{sample}/{assay}.rev.bam",
    params:
      outdir = "{data}/split_bams/{sample}/",
      outpre = "{assay}",
      scr = _pick_split_script,
      job_name = "splitbam",
      memory = "select[mem>8] rusage[mem=8] span[hosts=1]",
    log:
      "log/{sample}_bamsplit.txt"
    threads:
      10
    resources: all_threads=10
    shell:
      """
      bash {params.scr} {input} {params.outdir} {params.outpre} {threads}
      """

def _get_blacklisted_chroms(wildcards):
  
  bad_chroms = set()
  with open(CONTAMINATION_BED) as f:
    for line in f:
      bad_chroms.add(line.split("\t")[0])

  return " ".join(list(bad_chroms))

rule deeptools_fwd_comp:
    """
    generate normalized coverage ds and ssRNA bams
    """
    input:
      total =
      "{data}/split_bams/{samples}/{samples}_total_nascent_RNA_unique.bam.fwd.bam",
      test = "{data}/split_bams/{samples}/{samples}_{rna_type}_unique.bam.fwd.bam"
    output:
      bw = "{data}/norm_bw/{samples}/{samples}_{rna_type}_{chrom}_fwd.bw",
    params:
      badchroms = _get_blacklisted_chroms,
      job_name = "deeptools_norm.{samples}",
      memory = "select[mem>50] rusage[mem=50] span[hosts=1]",
    log:
      "log/norm_bw/{samples}_fwd.txt"
    threads:
      6
    resources: all_threads=6
    shell:
      """
      bamCompare \
        -b1 {input.test} \
        -b2 {input.total} \
        -o {output.bw} \
        -of "bigwig" \
        --scaleFactorsMethod readCount \
        --operation ratio \
        -bs 1 \
        --region {wildcards.chrom} \
        -ignore {params.badchroms} \
        --skipNAs \
        -bl {CONTAMINATION_BED} \
        -p 5 \
        -v 
      """

rule deeptools_rev_comp:
    """
    generate normalized coverage ds and ssRNA bams
    """
    input:
      total =
      "{data}/split_bams/{samples}/{samples}_total_nascent_RNA_unique.bam.rev.bam",
      test =
      "{data}/split_bams/{samples}/{samples}_{rna_type}_unique.bam.rev.bam"
    output:
      bw = "{data}/norm_bw/{samples}/{samples}_{rna_type}_{chrom}_rev.bw",
    params:
      badchroms = _get_blacklisted_chroms,
      job_name = "deeptools_norm.{samples}",
      memory = "select[mem>50] rusage[mem=50] span[hosts=1]",
    log:
      "log/norm_bw/{samples}_rev.txt"
    threads:
      8
    resources: all_threads=8
    shell:
      """
      bamCompare \
        -b1 {input.test} \
        -b2 {input.total} \
        -o {output.bw} \
        -of "bigwig" \
        --scaleFactorsMethod readCount \
        --operation ratio \
        -bs 1 \
        --region {wildcards.chrom} \
        --skipNAs \
        -ignore {params.badchroms} \
        -bl {CONTAMINATION_BED} \
        -p 5 \
        -v 
      """
